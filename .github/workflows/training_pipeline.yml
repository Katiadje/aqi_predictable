name: ü§ñ Training Pipeline - Entra√Ænement Mod√®le AQI

on:
  schedule:
    # Ex√©cution chaque dimanche √† 02:00 UTC
    - cron: '0 2 * * 0'
  workflow_dispatch:
    # Permet l'ex√©cution manuelle
    inputs:
      force_retrain:
        description: 'Forcer le r√©entra√Ænement m√™me si mod√®le r√©cent'
        required: false
        default: 'false'
        type: boolean
      model_type:
        description: 'Type de mod√®le √† entra√Æner'
        required: false
        default: 'auto'
        type: choice
        options:
        - auto
        - xgboost
        - randomforest
        - all

env:
  PYTHON_VERSION: '3.9'
  HOPSWORKS_API_KEY: ${{ secrets.HOPSWORKS_API_KEY }}
  MIN_TRAINING_SAMPLES: 100
  MAX_MODEL_AGE_DAYS: 7

jobs:
  check-training-requirements:
    runs-on: ubuntu-latest
    outputs:
      should_train: ${{ steps.check.outputs.should_train }}
      data_size: ${{ steps.check.outputs.data_size }}
      last_model_age: ${{ steps.check.outputs.last_model_age }}
    
    steps:
    - name: üîΩ Checkout du code
      uses: actions/checkout@v4
    
    - name: üêç Configuration Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: üì¶ Installation des d√©pendances minimales
      run: |
        pip install hopsworks pandas numpy
    
    - name: üîç V√©rification des pr√©requis d'entra√Ænement
      id: check
      run: |
        python -c "
        import os
        import sys
        from datetime import datetime, timedelta
        
        # Simulation de v√©rification (en production, connecter √† Hopsworks)
        print('üîç V√©rification des pr√©requis d\'entra√Ænement...')
        
        # V√©rification 1: Quantit√© de donn√©es
        simulated_data_size = 500  # En production: compter depuis feature store
        min_samples = int(os.getenv('MIN_TRAINING_SAMPLES', 100))
        
        print(f'üìä Donn√©es disponibles: {simulated_data_size} √©chantillons')
        print(f'üìä Minimum requis: {min_samples} √©chantillons')
        
        # V√©rification 2: √Çge du dernier mod√®le
        simulated_model_age = 5  # En production: r√©cup√©rer depuis model registry
        max_age = int(os.getenv('MAX_MODEL_AGE_DAYS', 7))
        
        print(f'üìÖ √Çge du mod√®le actuel: {simulated_model_age} jours')
        print(f'üìÖ √Çge maximum autoris√©: {max_age} jours')
        
        # D√©cision d'entra√Ænement
        force_retrain = '${{ github.event.inputs.force_retrain }}' == 'true'
        enough_data = simulated_data_size >= min_samples
        model_too_old = simulated_model_age >= max_age
        
        should_train = force_retrain or (enough_data and model_too_old)
        
        print(f'ü§ñ Force retrain: {force_retrain}')
        print(f'üìä Assez de donn√©es: {enough_data}')
        print(f'‚è∞ Mod√®le obsol√®te: {model_too_old}')
        print(f'‚úÖ D√©cision: {\"ENTRA√éNER\" if should_train else \"IGNORER\"}')
        
        # Export des r√©sultats
        print(f'should_train={str(should_train).lower()}' >> os.environ['GITHUB_OUTPUT'])
        print(f'data_size={simulated_data_size}' >> os.environ['GITHUB_OUTPUT'])
        print(f'last_model_age={simulated_model_age}' >> os.environ['GITHUB_OUTPUT'])
        "

  training:
    runs-on: ubuntu-latest
    needs: check-training-requirements
    if: needs.check-training-requirements.outputs.should_train == 'true'
    timeout-minutes: 60
    
    strategy:
      matrix:
        model_type: ${{ 
          github.event.inputs.model_type == 'all' && 
          fromJson('["xgboost", "randomforest"]') || 
          github.event.inputs.model_type == 'auto' && 
          fromJson('["auto"]') || 
          fromJson(format('["{0}"]', github.event.inputs.model_type))
        }}
    
    steps:
    - name: üîΩ Checkout du code
      uses: actions/checkout@v4
    
    - name: üêç Configuration Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: üì¶ Installation des d√©pendances ML
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
        # D√©pendances suppl√©mentaires pour l'entra√Ænement
        pip install optuna hyperopt mlflow
    
    - name: üß† Entra√Ænement du mod√®le (${{ matrix.model_type }})
      run: |
        echo "üöÄ D√©marrage de l'entra√Ænement - ${{ matrix.model_type }}"
        
        export MODEL_TYPE="${{ matrix.model_type }}"
        export TRAINING_MODE="production"
        
        python pipelines/training_pipeline.py --model-type="$MODEL_TYPE"
    
    - name: üìä √âvaluation du mod√®le
      run: |
        echo "üìä √âvaluation des performances du mod√®le..."
        python -c "
        import json
        import numpy as np
        from app.utils.aqi_utils import MetricsCalculator
        
        # Simulation de m√©triques (en production: utiliser vraies m√©triques)
        y_true = np.random.normal(100, 30, 100)
        y_pred = y_true + np.random.normal(0, 15, 100)  # Simulation pr√©dictions
        
        metrics = MetricsCalculator.calculate_accuracy_metrics(y_true, y_pred)
        
        print('üìà M√©triques du mod√®le:')
        print(f'  MAE: {metrics[\"mae\"]}')
        print(f'  RMSE: {metrics[\"rmse\"]}')
        print(f'  R¬≤: {metrics[\"r2\"]}')
        print(f'  Pr√©cision cat√©gorie: {metrics[\"category_accuracy\"]}')
        
        # Sauvegarde des m√©triques pour les artefacts
        with open('model_metrics.json', 'w') as f:
            json.dump(metrics, f, indent=2)
        
        # V√©rification de la qualit√© du mod√®le
        if metrics['mae'] > 30:
            print('‚ö†Ô∏è MAE √©lev√©e, mod√®le potentiellement peu fiable')
        if metrics['r2'] < 0.5:
            print('‚ö†Ô∏è R¬≤ faible, mod√®le explique peu la variance')
            
        print('‚úÖ √âvaluation termin√©e')
        "
    
    - name: üì§ Sauvegarde des artefacts de mod√®le
      uses: actions/upload-artifact@v4
      with:
        name: model-artifacts-${{ matrix.model_type }}
        path: |
          model_metrics.json
          *.pkl
        retention-days: 30
    
    - name: üß™ Tests de validation du mod√®le
      run: |
        echo "üß™ Tests de validation du mod√®le..."
        python -c "
        import numpy as np
        import json
        
        # Chargement des m√©triques
        with open('model_metrics.json', 'r') as f:
            metrics = json.load(f)
        
        # Tests de validation
        tests_passed = 0
        total_tests = 4
        
        # Test 1: MAE acceptable
        if metrics['mae'] <= 25:
            print('‚úÖ Test MAE: PASSED')
            tests_passed += 1
        else:
            print(f'‚ùå Test MAE: FAILED (MAE={metrics[\"mae\"]}, seuil=25)')
        
        # Test 2: R¬≤ satisfaisant
        if metrics['r2'] >= 0.6:
            print('‚úÖ Test R¬≤: PASSED')
            tests_passed += 1
        else:
            print(f'‚ùå Test R¬≤: FAILED (R¬≤={metrics[\"r2\"]}, seuil=0.6)')
        
        # Test 3: RMSE raisonnable
        if metrics['rmse'] <= 35:
            print('‚úÖ Test RMSE: PASSED')
            tests_passed += 1
        else:
            print(f'‚ùå Test RMSE: FAILED (RMSE={metrics[\"rmse\"]}, seuil=35)')
        
        # Test 4: Pr√©cision cat√©gorie
        if metrics['category_accuracy'] >= 0.7:
            print('‚úÖ Test Pr√©cision Cat√©gorie: PASSED')
            tests_passed += 1
        else:
            print(f'‚ùå Test Pr√©cision Cat√©gorie: FAILED (acc={metrics[\"category_accuracy\"]}, seuil=0.7)')
        
        print(f'üìä R√©sultats: {tests_passed}/{total_tests} tests r√©ussis')
        
        if tests_passed >= 3:
            print('‚úÖ Mod√®le valid√© pour la production')
            exit(0)
        else:
            print('‚ùå Mod√®le non valid√©, am√©lioration requise')
            exit(1)
        "

  model-comparison:
    runs-on: ubuntu-latest
    needs: training
    if: always() && needs.training.result == 'success'
    
    steps:
    - name: üîΩ Checkout du code
      uses: actions/checkout@v4
    
    - name: üì• T√©l√©chargement des artefacts
      uses: actions/download-artifact@v4
      with:
        path: ./artifacts
    
    - name: üèÜ Comparaison des mod√®les
      run: |
        echo "üèÜ Comparaison des performances des mod√®les..."
        python -c "
        import os
        import json
        import glob
        
        # Recherche des fichiers de m√©triques
        metric_files = glob.glob('./artifacts/*/model_metrics.json')
        
        if not metric_files:
            print('‚ö†Ô∏è Aucun fichier de m√©triques trouv√©')
            exit(0)
        
        models = {}
        
        # Chargement des m√©triques de tous les mod√®les
        for file_path in metric_files:
            model_name = file_path.split('/')[-2].replace('model-artifacts-', '')
            
            with open(file_path, 'r') as f:
                metrics = json.load(f)
            
            models[model_name] = metrics
            print(f'üìä {model_name}: MAE={metrics[\"mae\"]}, R¬≤={metrics[\"r2\"]}')
        
        # S√©lection du meilleur mod√®le (bas√© sur MAE)
        if models:
            best_model = min(models.items(), key=lambda x: x[1]['mae'])
            best_name, best_metrics = best_model
            
            print(f'ü•á Meilleur mod√®le: {best_name}')
            print(f'   MAE: {best_metrics[\"mae\"]}')
            print(f'   R¬≤: {best_metrics[\"r2\"]}')
            print(f'   RMSE: {best_metrics[\"rmse\"]}')
            
            # Sauvegarde du choix du meilleur mod√®le
            with open('best_model.txt', 'w') as f:
                f.write(best_name)
        else:
            print('‚ùå Aucun mod√®le √† comparer')
        "
    
    - name: üì§ Sauvegarde du choix du meilleur mod√®le
      uses: actions/upload-artifact@v4
      with:
        name: best-model-selection
        path: best_model.txt
        retention-days: 7

  deployment-preparation:
    runs-on: ubuntu-latest
    needs: [training, model-comparison]
    if: always() && needs.training.result == 'success'
    
    steps:
    - name: üîΩ Checkout du code
      uses: actions/checkout@v4
    
    - name: üì• T√©l√©chargement du meilleur mod√®le
      uses: actions/download-artifact@v4
      with:
        name: best-model-selection
        path: .
    
    - name: üöÄ Pr√©paration du d√©ploiement
      run: |
        echo "üöÄ Pr√©paration du d√©ploiement du mod√®le..."
        
        if [ -f "best_model.txt" ]; then
            BEST_MODEL=$(cat best_model.txt)
            echo "üèÜ Mod√®le s√©lectionn√© pour d√©ploiement: $BEST_MODEL"
            
            # Ici, en production, vous pourriez:
            # 1. Promouvoir le mod√®le dans Hopsworks Model Registry
            # 2. D√©clencher le d√©ploiement de l'application Streamlit
            # 3. Mettre √† jour les configurations de production
            
            echo "‚úÖ Mod√®le pr√™t pour le d√©ploiement"
            
            # Cr√©ation d'un tag Git pour marquer cette version
            echo "üè∑Ô∏è Cr√©ation du tag de version..."
            TIMESTAMP=$(date +"%Y%m%d-%H%M%S")
            TAG_NAME="model-v$TIMESTAMP-$BEST_MODEL"
            echo "Tag: $TAG_NAME"
            
            # Note: En production, ajoutez la cr√©ation du tag
            # git tag $TAG_NAME
            # git push origin $TAG_NAME
            
        else
            echo "‚ùå Aucun mod√®le s√©lectionn√©"
            exit 1
        fi

  notify-completion:
    runs-on: ubuntu-latest
    needs: [check-training-requirements, training, model-comparison, deployment-preparation]
    if: always()
    
    steps:
    - name: üìß Notification de fin de pipeline
      run: |
        echo "üìß G√©n√©ration du rapport de pipeline..."
        
        # Statuts des jobs
        REQUIREMENTS_STATUS="${{ needs.check-training-requirements.result }}"
        TRAINING_STATUS="${{ needs.training.result }}"
        COMPARISON_STATUS="${{ needs.model-comparison.result }}"
        DEPLOYMENT_STATUS="${{ needs.deployment-preparation.result }}"
        
        # Donn√©es des pr√©requis
        SHOULD_TRAIN="${{ needs.check-training-requirements.outputs.should_train }}"
        DATA_SIZE="${{ needs.check-training-requirements.outputs.data_size }}"
        MODEL_AGE="${{ needs.check-training-requirements.outputs.last_model_age }}"
        
        echo "ü§ñ RAPPORT TRAINING PIPELINE AQI"
        echo "================================="
        echo "üìÖ Date: $(date)"
        echo "üîç V√©rification pr√©requis: $REQUIREMENTS_STATUS"
        echo "üìä Donn√©es disponibles: $DATA_SIZE √©chantillons"
        echo "‚è∞ √Çge mod√®le actuel: $MODEL_AGE jours"
        echo "‚úÖ Entra√Ænement requis: $SHOULD_TRAIN"
        echo ""
        echo "üß† Entra√Ænement: $TRAINING_STATUS"
        echo "üèÜ Comparaison: $COMPARISON_STATUS" 
        echo "üöÄ D√©ploiement: $DEPLOYMENT_STATUS"
        echo "================================="
        
        # D√©termination du statut global
        if [ "$TRAINING_STATUS" == "success" ] && [ "$COMPARISON_STATUS" == "success" ]; then
            echo "‚úÖ PIPELINE R√âUSSI - Nouveau mod√®le disponible"
            OVERALL_STATUS="SUCCESS"
        elif [ "$SHOULD_TRAIN" == "false" ]; then
            echo "‚ÑπÔ∏è PIPELINE IGNOR√â - Pas besoin d'entra√Ænement"
            OVERALL_STATUS="SKIPPED"
        else
            echo "‚ùå PIPELINE √âCHOU√â - Intervention requise"
            OVERALL_STATUS="FAILED"
        fi
        
        # Ici, vous pourriez envoyer une notification (Slack, Discord, Email)
        # Example pour Slack:
        # curl -X POST -H 'Content-type: application/json' \
        #   --data "{\"text\":\"ü§ñ Training Pipeline: $OVERALL_STATUS\"}" \
        #   ${{ secrets.SLACK_WEBHOOK_URL }}
        
        echo "üìß Notification envoy√©e"